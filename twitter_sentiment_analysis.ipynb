{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment         ids                          date      flag  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df= pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", names = columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>670935</td>\n",
       "      <td>0</td>\n",
       "      <td>@TheRealScarab PA system bugs are a bummer, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>649589</td>\n",
       "      <td>0</td>\n",
       "      <td>oh daaamnnn! the firemen ball's on the 14th an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2028</td>\n",
       "      <td>0</td>\n",
       "      <td>I don't want to be cold in April, but I am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>669856</td>\n",
       "      <td>0</td>\n",
       "      <td>@JaredOngie haha its too cold down here  bt ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>458897</td>\n",
       "      <td>0</td>\n",
       "      <td>Upset I can't find my CHI!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1006064</td>\n",
       "      <td>4</td>\n",
       "      <td>Off in a bit to meet Spuds new family at his f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1497567</td>\n",
       "      <td>4</td>\n",
       "      <td>watching videos in youtube about Drake Bell..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1506693</td>\n",
       "      <td>4</td>\n",
       "      <td>who wants to go to the laker parade with me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1061627</td>\n",
       "      <td>4</td>\n",
       "      <td>@BELAI83 i'm great!!!hang out but i have to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1152613</td>\n",
       "      <td>4</td>\n",
       "      <td>just got home from Julius grad party</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  sentiment                                               text\n",
       "0       670935          0  @TheRealScarab PA system bugs are a bummer, so...\n",
       "1       649589          0  oh daaamnnn! the firemen ball's on the 14th an...\n",
       "2         2028          0        I don't want to be cold in April, but I am \n",
       "3       669856          0  @JaredOngie haha its too cold down here  bt ot...\n",
       "4       458897          0                        Upset I can't find my CHI! \n",
       "...        ...        ...                                                ...\n",
       "99995  1006064          4  Off in a bit to meet Spuds new family at his f...\n",
       "99996  1497567          4     watching videos in youtube about Drake Bell.. \n",
       "99997  1506693          4       who wants to go to the laker parade with me \n",
       "99998  1061627          4  @BELAI83 i'm great!!!hang out but i have to co...\n",
       "99999  1152613          4              just got home from Julius grad party \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled = df.groupby('sentiment')[['sentiment', 'text']].sample(n=50000, random_state=1)\n",
    "df_sampled.reset_index(inplace=True)\n",
    "df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='sentiment'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sampled.groupby(['sentiment']).size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index        0\n",
       "sentiment    0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /home/kevinzhu224/.local/lib/python3.8/site-packages (1.9.2)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from wordcloud) (7.0.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/kevinzhu224/.local/lib/python3.8/site-packages (from wordcloud) (1.24.4)\n",
      "Requirement already satisfied: matplotlib in /home/kevinzhu224/.local/lib/python3.8/site-packages (from wordcloud) (3.7.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/kevinzhu224/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/kevinzhu224/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /home/kevinzhu224/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (6.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/kevinzhu224/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (4.44.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kevinzhu224/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/kevinzhu224/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/kevinzhu224/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kevinzhu224/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (23.2)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/kevinzhu224/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib->wordcloud) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "# plt.figure(figsize=(20, 10))\n",
    "\n",
    "# # Combine all text entries from the 'text' column of your dataframe into a single string\n",
    "# text_combined = \" \".join(cat for cat in df_sampled.text)\n",
    "# word_cloud = WordCloud(\n",
    "#     collocations=False, \n",
    "#     background_color='white', \n",
    "#     width=2000, \n",
    "#     height=1000\n",
    "# ).generate(text_combined)\n",
    "\n",
    "# # Display the generated Word Cloud\n",
    "# plt.imshow(word_cloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")  # Turn off the axis numbers and labels\n",
    "# plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing \n",
    "\n",
    "1. Lower case\n",
    "2. Removing urls\n",
    "3. Removing usernames\n",
    "4. Replace emojis\n",
    "5. Remove the chat words and numbers(e.g lol to laugh out loud , 1 to one)\n",
    "6. replace contractions\n",
    "7. Remove punctuations\n",
    "8. Lemmatization and replace consecutive letters\n",
    "9. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acronym</th>\n",
       "      <th>expansion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2day</td>\n",
       "      <td>today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2m2h</td>\n",
       "      <td>too much too handle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2moro</td>\n",
       "      <td>tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2nite</td>\n",
       "      <td>tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4eae</td>\n",
       "      <td>for ever and ever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  acronym            expansion\n",
       "0    2day                today\n",
       "1    2m2h  too much too handle\n",
       "2   2moro             tomorrow\n",
       "3   2nite              tonight\n",
       "4    4eae    for ever and ever"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from num2words import num2words\n",
    "slangDf = pd.read_csv(\"slang.csv\")\n",
    "slangDf=slangDf[['acronym','expansion']]\n",
    "slangDf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'laughing out loud'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_chat_words(text):\n",
    "    normal_word=slangDf[slangDf['acronym'].isin([text])]['expansion'].values\n",
    "    if len(normal_word)>=1:\n",
    "        if text=='lol':\n",
    "            return normal_word[1]\n",
    "        else:\n",
    "            return normal_word[0]\n",
    "    elif text.isnumeric():\n",
    "        return num2words(text)\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "replace_chat_words('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import contractions as con\n",
    "import string\n",
    "import en_core_web_lg\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_lg\n",
    "from autocorrect import Speller\n",
    "\n",
    "nlp=en_core_web_lg.load()\n",
    "speller=Speller(lang='en')\n",
    "stop_words=nlp.Defaults.stop_words\n",
    "\n",
    "def preprocessingText(text):\n",
    "  text = text.lower()\n",
    "  # Remove urls\n",
    "  text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "  # # Remove usernames\n",
    "  text = re.sub(r'@[^\\s]+','', text)\n",
    "  # # Replace all emojis from the emoji shortcodes\n",
    "  text = emoji.demojize(text)\n",
    "  # # Replace chat words and numbers\n",
    "  text = \" \".join([replace_chat_words(word) for word in text.split()])\n",
    "  # Replace contraction words\n",
    "  text=con.fix(text)\n",
    "  # Remove punctuations\n",
    "  text = \"\".join([i for i in text if i not in string.punctuation])\n",
    "  # Replace 3 or more consecutive letters by 1 letter and lemmatizing the words\n",
    "  text = \" \".join([re.sub(r\"(.)\\1\\1+\", r\"\\1\", str(token)) if token.pos_ in [\"PROPN\", 'NOUN'] else token.lemma_ for token in nlp(text)])\n",
    "  # Replace misspelled words\n",
    "  text=speller(text)\n",
    "  # Remove stopwords\n",
    "  text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "  text = text.strip()\n",
    "\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 21:04:55 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.234.128 instead (on interface ens33)\n",
      "23/11/15 21:04:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/11/15 21:04:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext,Row \n",
    "\n",
    "conf=SparkConf()\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "sc=SparkContext(conf=conf)\n",
    "sqlContext=SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sqlContext.read.csv('training.1600000.processed.noemoticon.csv',header=True)\n",
    "df=df.rdd\n",
    "df=df.map(lambda x:(x[0],x[5]))\n",
    "df_processed=df.map(lambda x:(0 if x[0]=='0' else 1,preprocessingText(x[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_processed=df_processed.toDF([\"sentiment\", \"text\"]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 21:16:22 ERROR Executor: Exception in task 6.0 in stage 2.0 (TID 8)/ 8]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5822/1261530550.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in preprocessingText\n",
      "  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in <listcomp>\n",
      "  File \"/tmp/ipykernel_5822/3417128140.py\", line 9, in replace_chat_words\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/__init__.py\", line 95, in num2words\n",
      "    number = converter.str_to_number(number)\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/base.py\", line 101, in str_to_number\n",
      "    return Decimal(value)\n",
      "decimal.InvalidOperation: [<class 'decimal.ConversionSyntax'>]\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/15 21:16:22 WARN TaskSetManager: Lost task 6.0 in stage 2.0 (TID 8) (192.168.234.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5822/1261530550.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in preprocessingText\n",
      "  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in <listcomp>\n",
      "  File \"/tmp/ipykernel_5822/3417128140.py\", line 9, in replace_chat_words\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/__init__.py\", line 95, in num2words\n",
      "    number = converter.str_to_number(number)\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/base.py\", line 101, in str_to_number\n",
      "    return Decimal(value)\n",
      "decimal.InvalidOperation: [<class 'decimal.ConversionSyntax'>]\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/11/15 21:16:22 ERROR TaskSetManager: Task 6 in stage 2.0 failed 1 times; aborting job\n",
      "/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:137: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.\n",
      "  An error occurred while calling o69.getResult.\n",
      ": org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:97)\n",
      "\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:93)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 8) (192.168.234.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5822/1261530550.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in preprocessingText\n",
      "  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in <listcomp>\n",
      "  File \"/tmp/ipykernel_5822/3417128140.py\", line 9, in replace_chat_words\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/__init__.py\", line 95, in num2words\n",
      "    number = converter.str_to_number(number)\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/base.py\", line 101, in str_to_number\n",
      "    return Decimal(value)\n",
      "decimal.InvalidOperation: [<class 'decimal.ConversionSyntax'>]\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3629)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3633)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3610)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3610)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3609)\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:139)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:141)\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:136)\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:113)\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:107)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:68)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:68)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5822/1261530550.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in preprocessingText\n",
      "  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in <listcomp>\n",
      "  File \"/tmp/ipykernel_5822/3417128140.py\", line 9, in replace_chat_words\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/__init__.py\", line 95, in num2words\n",
      "    number = converter.str_to_number(number)\n",
      "  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/base.py\", line 101, in str_to_number\n",
      "    return Decimal(value)\n",
      "decimal.InvalidOperation: [<class 'decimal.ConversionSyntax'>]\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o69.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:97)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:93)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 8) (192.168.234.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_5822/1261530550.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in preprocessingText\n  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in <listcomp>\n  File \"/tmp/ipykernel_5822/3417128140.py\", line 9, in replace_chat_words\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/__init__.py\", line 95, in num2words\n    number = converter.str_to_number(number)\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/base.py\", line 101, in str_to_number\n    return Decimal(value)\ndecimal.InvalidOperation: [<class 'decimal.ConversionSyntax'>]\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3629)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3633)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3610)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3610)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3609)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:139)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:141)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:136)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:113)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:68)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:68)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_5822/1261530550.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in preprocessingText\n  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in <listcomp>\n  File \"/tmp/ipykernel_5822/3417128140.py\", line 9, in replace_chat_words\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/__init__.py\", line 95, in num2words\n    number = converter.str_to_number(number)\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/base.py\", line 101, in str_to_number\n    return Decimal(value)\ndecimal.InvalidOperation: [<class 'decimal.ConversionSyntax'>]\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/kevinzhu224/CS5344/twitter_sentiment_analysis.ipynb å•å…ƒæ ¼ 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kevinzhu224/CS5344/twitter_sentiment_analysis.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_processed\u001b[39m.\u001b[39;49mtoPandas()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:108\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39m# Rename columns to avoid duplicated column names.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m tmp_column_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mcol_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))]\n\u001b[0;32m--> 108\u001b[0m batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoDF(\u001b[39m*\u001b[39;49mtmp_column_names)\u001b[39m.\u001b[39;49m_collect_as_arrow()\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batches) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    110\u001b[0m     table \u001b[39m=\u001b[39m pyarrow\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_batches(batches)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:246\u001b[0m, in \u001b[0;36mPandasConversionMixin._collect_as_arrow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(_load_from_socket((port, auth_secret), ArrowCollectSerializer()))\n\u001b[1;32m    244\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     \u001b[39m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     jsocket_auth_server\u001b[39m.\u001b[39;49mgetResult()\n\u001b[1;32m    248\u001b[0m \u001b[39m# Separate RecordBatches from batch order indices in results\u001b[39;00m\n\u001b[1;32m    249\u001b[0m batches \u001b[39m=\u001b[39m results[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o69.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:97)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:93)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 8) (192.168.234.128 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_5822/1261530550.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in preprocessingText\n  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in <listcomp>\n  File \"/tmp/ipykernel_5822/3417128140.py\", line 9, in replace_chat_words\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/__init__.py\", line 95, in num2words\n    number = converter.str_to_number(number)\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/base.py\", line 101, in str_to_number\n    return Decimal(value)\ndecimal.InvalidOperation: [<class 'decimal.ConversionSyntax'>]\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3629)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3633)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3610)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3610)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3609)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:139)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:141)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:136)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:113)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:68)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:68)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_5822/1261530550.py\", line 4, in <lambda>\n  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in preprocessingText\n  File \"/tmp/ipykernel_5822/898944990.py\", line 23, in <listcomp>\n  File \"/tmp/ipykernel_5822/3417128140.py\", line 9, in replace_chat_words\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/__init__.py\", line 95, in num2words\n    number = converter.str_to_number(number)\n  File \"/home/kevinzhu224/.local/lib/python3.8/site-packages/num2words/base.py\", line 101, in str_to_number\n    return Decimal(value)\ndecimal.InvalidOperation: [<class 'decimal.ConversionSyntax'>]\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 21:16:24 WARN TaskSetManager: Lost task 7.0 in stage 2.0 (TID 9) (192.168.234.128 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/15 21:16:25 WARN PythonRunner: Incomplete task 0.0 in stage 2 (TID 2) interrupted: Attempting to kill Python Worker\n",
      "23/11/15 21:16:25 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (192.168.234.128 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/15 21:16:25 WARN PythonRunner: Incomplete task 1.0 in stage 2 (TID 3) interrupted: Attempting to kill Python Worker\n",
      "23/11/15 21:16:25 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 3) (192.168.234.128 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/15 21:16:25 WARN PythonRunner: Incomplete task 5.0 in stage 2 (TID 7) interrupted: Attempting to kill Python Worker\n",
      "23/11/15 21:16:25 WARN TaskSetManager: Lost task 5.0 in stage 2.0 (TID 7) (192.168.234.128 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/15 21:16:25 WARN PythonRunner: Incomplete task 4.0 in stage 2 (TID 6) interrupted: Attempting to kill Python Worker\n",
      "23/11/15 21:16:25 WARN TaskSetManager: Lost task 4.0 in stage 2.0 (TID 6) (192.168.234.128 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/15 21:16:25 WARN PythonRunner: Incomplete task 2.0 in stage 2 (TID 4) interrupted: Attempting to kill Python Worker\n",
      "23/11/15 21:16:25 WARN TaskSetManager: Lost task 2.0 in stage 2.0 (TID 4) (192.168.234.128 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/15 21:16:25 WARN PythonRunner: Incomplete task 3.0 in stage 2 (TID 5) interrupted: Attempting to kill Python Worker\n",
      "23/11/15 21:16:25 WARN TaskSetManager: Lost task 3.0 in stage 2.0 (TID 5) (192.168.234.128 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "df_processed.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
