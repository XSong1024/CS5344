{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df= pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", names = columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df.groupby('sentiment')[['sentiment', 'text']].sample(n=50000, random_state=1)\n",
    "df_sampled.reset_index(inplace=True)\n",
    "df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.groupby(['sentiment']).size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "# plt.figure(figsize=(20, 10))\n",
    "\n",
    "# # Combine all text entries from the 'text' column of your dataframe into a single string\n",
    "# text_combined = \" \".join(cat for cat in df_sampled.text)\n",
    "# word_cloud = WordCloud(\n",
    "#     collocations=False, \n",
    "#     background_color='white', \n",
    "#     width=2000, \n",
    "#     height=1000\n",
    "# ).generate(text_combined)\n",
    "\n",
    "# # Display the generated Word Cloud\n",
    "# plt.imshow(word_cloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")  # Turn off the axis numbers and labels\n",
    "# plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing \n",
    "\n",
    "1. Lower case\n",
    "2. Removing urls\n",
    "3. Removing usernames\n",
    "4. Replace emojis\n",
    "5. Remove the chat words and numbers(e.g lol to laugh out loud , 1 to one)\n",
    "6. replace contractions\n",
    "7. Remove punctuations\n",
    "8. Lemmatization and replace consecutive letters\n",
    "9. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "slangDf = pd.read_csv(\"slang.csv\")\n",
    "slangDf=slangDf[['acronym','expansion']]\n",
    "slangDf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_chat_words(text):\n",
    "    normal_word=slangDf[slangDf['acronym'].isin([text])]['expansion'].values\n",
    "    if len(normal_word)>=1:\n",
    "        if text=='lol':\n",
    "            return normal_word[1]\n",
    "        else:\n",
    "            return normal_word[0]\n",
    "    elif text.isnumeric():\n",
    "        return num2words(text)\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "replace_chat_words('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import contractions as con\n",
    "import string\n",
    "import en_core_web_lg\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_lg\n",
    "from autocorrect import Speller\n",
    "\n",
    "nlp=en_core_web_lg.load()\n",
    "speller=Speller(lang='en')\n",
    "stop_words=nlp.Defaults.stop_words\n",
    "\n",
    "def preprocessingText(text):\n",
    "  text = text.lower()\n",
    "  # Remove urls\n",
    "  text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "  # # Remove usernames\n",
    "  text = re.sub(r'@[^\\s]+','', text)\n",
    "  # # Replace all emojis from the emoji shortcodes\n",
    "  text = emoji.demojize(text)\n",
    "  # # Replace chat words and numbers\n",
    "  text = \" \".join([replace_chat_words(word) for word in text.split()])\n",
    "  # Replace contraction words\n",
    "  text=con.fix(text)\n",
    "  # Remove punctuations\n",
    "  text = \"\".join([i for i in text if i not in string.punctuation])\n",
    "  # Replace 3 or more consecutive letters by 1 letter and lemmatizing the words\n",
    "  text = \" \".join([re.sub(r\"(.)\\1\\1+\", r\"\\1\", str(token)) if token.pos_ in [\"PROPN\", 'NOUN'] else token.lemma_ for token in nlp(text)])\n",
    "  # Replace misspelled words\n",
    "  text=speller(text)\n",
    "  # Remove stopwords\n",
    "  text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "  text = text.strip()\n",
    "\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf=SparkConf()\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "sc=SparkContext(conf=conf)\n",
    "sqlContext=SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sqlContext.read.csv('training.1600000.processed.noemoticon.csv',header=True)\n",
    "df=df.rdd\n",
    "df=df.map(lambda x:(x[0],x[5]))\n",
    "df_processed=df.map(lambda x:(0 if x[0]=='0' else 1,preprocessingText(x[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed=df_processed.toDF([\"sentiment\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
